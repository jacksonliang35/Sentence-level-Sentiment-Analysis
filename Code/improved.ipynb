{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18d9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from random import shuffle\n",
    "\n",
    "pos_text_dir = '../Data/review_polarity/txt_sentoken/pos/'\n",
    "neg_text_dir = '../Data/review_polarity/txt_sentoken/neg/'\n",
    "\n",
    "pos_sentences = []\n",
    "pos_sentences_postag = []\n",
    "for file in os.listdir(pos_text_dir):\n",
    "    with open(os.path.join(pos_text_dir,file), 'r') as f:\n",
    "        for lines in f:\n",
    "            lines = re.sub('n\\'t', ' not', lines)\n",
    "#             lines = re.sub('\\d*(\\.\\d*)?', '', lines)\n",
    "            sent = word_tokenize(lines)\n",
    "            pos_sentences.append(sent)\n",
    "            # Also get POS tags for each word\n",
    "            pos_sentences_postag.append(pos_tag(sent))\n",
    "\n",
    "neg_sentences = []\n",
    "neg_sentences_postag = []\n",
    "for file in os.listdir(neg_text_dir):\n",
    "    with open(os.path.join(neg_text_dir,file), 'r') as f:\n",
    "        for lines in f:\n",
    "            lines = re.sub('n\\'t', ' not', lines)\n",
    "#             lines = re.sub('\\d*(\\.\\d*)?', '', lines)\n",
    "            sent = word_tokenize(lines)\n",
    "            neg_sentences.append(sent)\n",
    "            # Also get POS tags for each word\n",
    "            neg_sentences_postag.append(pos_tag(sent))\n",
    "\n",
    "npos = len(pos_sentences)\n",
    "nneg = len(neg_sentences)\n",
    "\n",
    "# Part 1: Negation Handling\n",
    "HANDLE_NEGATION = False\n",
    "if HANDLE_NEGATION:\n",
    "    for s in range(npos):\n",
    "        negation_state = False\n",
    "        for w in range(len(pos_sentences[s])):\n",
    "            if pos_sentences[s][w] in set(string.punctuation).union(['only', 'that', 'what', 'when', 'which', 'where', 'how', 'who']):\n",
    "                negation_state = False\n",
    "                continue\n",
    "            if pos_sentences[s][w] in set(['no','not','never','less','rarely','barely','seldom']):\n",
    "                negation_state = True\n",
    "                continue\n",
    "            if negation_state and pos_sentences_postag[s][w][1] in set(['JJ','JJR','JJS','NN','NNP','NNS','RB','RBR','RBP','RP','VB','VBD','VBG','VBN','VBP','VBZ']):\n",
    "                pos_sentences[s][w] = pos_sentences[s][w] + '_NEG'\n",
    "    \n",
    "    for s in range(nneg):\n",
    "        negation_state = False\n",
    "        for w in range(len(neg_sentences[s])):\n",
    "            if neg_sentences[s][w] in set(string.punctuation).union(['only', 'that', 'what', 'when', 'which', 'where', 'how', 'who']):\n",
    "                negation_state = False\n",
    "                continue\n",
    "            if neg_sentences[s][w] in set(['no','not','never','less','rarely','barely','seldom']):\n",
    "                negation_state = True\n",
    "                continue\n",
    "            if negation_state and neg_sentences_postag[s][w][1] in set(['JJ','JJR','JJS','NN','NNP','NNS','RB','RBR','RBP','RP','VB','VBD','VBG','VBN','VBP','VBZ']):\n",
    "                neg_sentences[s][w] = neg_sentences[s][w] + '_NEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a18eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc41e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHUFFLE = True\n",
    "if RESHUFFLE:\n",
    "    pos_shuff_idx = list(range(len(pos_sentences)))\n",
    "    shuffle(pos_shuff_idx)\n",
    "    neg_shuff_idx = list(range(len(neg_sentences)))\n",
    "    shuffle(neg_shuff_idx)\n",
    "\n",
    "pos_sentences = [pos_sentences[s] for s in pos_shuff_idx]\n",
    "pos_sentences_postag = [pos_sentences_postag[s] for s in pos_shuff_idx]\n",
    "neg_sentences = [neg_sentences[s] for s in neg_shuff_idx]\n",
    "neg_sentences_postag = [neg_sentences_postag[s] for s in neg_shuff_idx]\n",
    "\n",
    "traindata = [pos_sentences[:npos//10*6],\n",
    "             pos_sentences_postag[:npos//10*6],\n",
    "             neg_sentences[:nneg//10*6],\n",
    "             neg_sentences_postag[:nneg//10*6]\n",
    "            ]\n",
    "valdata = [pos_sentences[npos//10*6:npos//10*8],\n",
    "           pos_sentences_postag[npos//10*6:npos//10*8],\n",
    "           neg_sentences[nneg//10*6:nneg//10*8],\n",
    "           neg_sentences_postag[nneg//10*6:nneg//10*8]\n",
    "          ]\n",
    "testdata = [pos_sentences[npos//10*8:],\n",
    "            pos_sentences_postag[npos//10*8:],\n",
    "            neg_sentences[nneg//10*8:],\n",
    "            neg_sentences_postag[nneg//10*8:]\n",
    "           ]\n",
    "\n",
    "sent_lab_set = [(0,1),(2,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20da6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True\n",
    "if TESTING:\n",
    "    for i in range(len(traindata)):\n",
    "        traindata[i] += valdata[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e529c35",
   "metadata": {},
   "source": [
    "Part 2: Pointwise Mutual Information (PMI) scores using root forms from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b4827",
   "metadata": {},
   "source": [
    "First for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d759e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "trainpos = [w for w in chain.from_iterable(traindata[sent_lab_set[0][0]])]\n",
    "trainneg = [w for w in chain.from_iterable(traindata[sent_lab_set[1][0]])]\n",
    "cntpos = Counter(trainpos)\n",
    "cntneg = Counter(trainneg)\n",
    "total_words = set(trainpos).union(set(trainneg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299607dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_uni_list = []\n",
    "uni_pmi_score = {}\n",
    "tot_num_pos = len(trainpos)\n",
    "tot_num_neg = len(trainneg)\n",
    "for w in total_words:\n",
    "    if cntpos[w] > 10 and cntneg[w] > 10:\n",
    "        # remove words with too few occurances\n",
    "        pmi_pos = np.log2(cntpos[w] * (tot_num_pos + tot_num_neg) / (cntpos[w] + cntneg[w]) / tot_num_pos)\n",
    "        pmi_neg = np.log2(cntneg[w] * (tot_num_pos + tot_num_neg) / (cntpos[w] + cntneg[w]) / tot_num_neg)\n",
    "        uni_pmi_score[w] = pmi_pos - pmi_neg\n",
    "        pmi_uni_list.append(w)\n",
    "pmi_uni_list = set(pmi_uni_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713bfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_uni_list = []\n",
    "uni_logprob = {}\n",
    "for w in total_words:\n",
    "    prob = (1+cntpos[w])/(2+cntpos[w]+cntneg[w])\n",
    "    prob_uni_list.append(w)\n",
    "    uni_logprob[w] = np.log(prob)\n",
    "prob_uni_list = set(prob_uni_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12565cf6",
   "metadata": {},
   "source": [
    "Same technique for bigrams (but without stemming)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ce9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpos_bi = []\n",
    "for sent in traindata[sent_lab_set[0][0]]:\n",
    "    trainpos_bi.extend(list(zip(sent[:-1],sent[1:])))\n",
    "\n",
    "trainneg_bi = []\n",
    "for sent in traindata[sent_lab_set[1][0]]:\n",
    "    trainneg_bi.extend(list(zip(sent[:-1],sent[1:])))\n",
    "\n",
    "cntpos_bi = Counter(trainpos_bi)\n",
    "cntneg_bi = Counter(trainneg_bi)\n",
    "total_words_bi = set(trainpos_bi).union(set(trainneg_bi))\n",
    "\n",
    "pmi_bi_list = []\n",
    "pmi_bi_score = {}\n",
    "tot_num_pos_bi = len(trainpos_bi)\n",
    "tot_num_neg_bi = len(trainneg_bi)\n",
    "for w in total_words_bi:\n",
    "    if cntpos_bi[w] > 5 and cntneg_bi[w] > 5:\n",
    "        # remove words with too few occurances\n",
    "        pmi_pos_bi = np.log2(cntpos_bi[w] * (tot_num_pos_bi + tot_num_neg_bi) / (cntpos_bi[w] + cntneg_bi[w]) / tot_num_pos_bi)\n",
    "        pmi_neg_bi = np.log2(cntneg_bi[w] * (tot_num_pos_bi + tot_num_neg_bi) / (cntpos_bi[w] + cntneg_bi[w]) / tot_num_neg_bi)\n",
    "        pmi_bi_score[w] = pmi_pos_bi - pmi_neg_bi\n",
    "        pmi_bi_list.append(w)\n",
    "pmi_bi_list = set(pmi_bi_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60fa09c",
   "metadata": {},
   "source": [
    "Part 3: Loading NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9d0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Unigram\n",
    "NRC_lex_uni = pd.read_table('../Data/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/AutomaticallyGeneratedLexicons/NRC-Emoticon-AffLexNegLex-v1.0/Emoticon-AFFLEX-NEGLEX-unigrams.txt',names=['w','score','npos','nneg']).dropna()\n",
    "NRC_uni_dict = NRC_lex_uni[['w','score']].set_index('w').T.to_dict()\n",
    "NRC_uni_keys = set(NRC_uni_dict.keys())\n",
    "NRC_uni_dict = {w:NRC_uni_dict[w]['score'] for w in NRC_uni_dict}\n",
    "\n",
    "for w in NRC_uni_keys:\n",
    "    if w.endswith('_NEGFIRST'):\n",
    "        if (w[:-9]+'_NEG') not in NRC_uni_keys:\n",
    "            NRC_uni_dict[w[:-9]+'_NEG'] = NRC_uni_dict[w]\n",
    "        else:\n",
    "            NRC_uni_dict[w[:-9]+'_NEG'] = (NRC_uni_dict[w] + NRC_uni_dict[w[:-9]+'_NEG']) / 2\n",
    "        del NRC_uni_dict[w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2024ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram\n",
    "NRC_lex_bi = pd.read_table('../Data/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/AutomaticallyGeneratedLexicons/NRC-Emoticon-AffLexNegLex-v1.0/Emoticon-AFFLEX-NEGLEX-bigrams.txt',names=['w','score','npos','nneg'],quoting=3).dropna()\n",
    "NRC_bi_dict = NRC_lex_bi[['w','score']].set_index('w').T.to_dict()\n",
    "temp_keys = set([tuple(w.split()) for w in NRC_bi_dict.keys()])\n",
    "NRC_bi_dict = {tuple(w.split()):NRC_bi_dict[w]['score'] for w in NRC_bi_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc1614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC_uni_keys = set(NRC_uni_dict.keys())\n",
    "NRC_bi_keys = set(NRC_bi_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92641f",
   "metadata": {},
   "source": [
    "Features for sentence classification:\n",
    "1. Number of positive and negative words.\n",
    "2. Sum, max, and min of sentiment scores (if available) of unigram and bigram tokens.\n",
    "3. Sum of sentiment scores (if available) from, in particular, adjectives, adverbs, verbs, and nouns, respectively.\n",
    "4. Sum of sentiment scores (if available) from NRC Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecb9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    feat = []\n",
    "    label = []\n",
    "    for comb in sent_lab_set:\n",
    "        for s in range(len(data[comb[0]])):\n",
    "            pos_num, neg_num, pos_pmi_num, neg_pmi_num = 0,0,0,0\n",
    "            uni_prob_sum, uni_sum, uni_max, uni_min = 0,0,0,0\n",
    "            bi_sum, bi_max, bi_min = 0,0,0\n",
    "            adj_sum, adv_sum, vb_sum, nn_sum = 0,0,0,0\n",
    "            adj_pmi_sum, adv_pmi_sum, vb_pmi_sum, nn_pmi_sum = 0,0,0,0\n",
    "            nrc_uni_sum, nrc_bi_sum = 0,0\n",
    "            for w1,pos1 in zip(data[comb[0]][s], data[comb[0]+1][s]):\n",
    "                # Use of unigram probs\n",
    "                if w1 in prob_uni_list:\n",
    "                    uni_prob_sum += uni_logprob[w1]\n",
    "                    if uni_logprob[w1] > -0.2:\n",
    "                        pos_num += 1\n",
    "                    elif uni_logprob[w1] < -0.2:\n",
    "                        neg_num += 1\n",
    "                    # Use of POS tags\n",
    "                    if pos1[1] in set(['JJ','JJR','JJS']):\n",
    "                        adj_sum += uni_logprob[w1]\n",
    "                    if pos1[1] in set(['NN','NNP','NNS']):\n",
    "                        nn_sum += uni_logprob[w1]\n",
    "                    if pos1[1] in set(['RB','RBR','RBP','RP']):\n",
    "                        adv_sum += uni_logprob[w1]\n",
    "                    if pos1[1] in set(['VB','VBD','VBG','VBN','VBP','VBZ']):\n",
    "                        vb_sum += uni_logprob[w1]\n",
    "                # Use of unigram PMI\n",
    "                if w1 in pmi_uni_list:\n",
    "                    uni_sum += uni_pmi_score[w1]\n",
    "                    if uni_pmi_score[w1] > uni_max:\n",
    "                        uni_max = uni_pmi_score[w1]\n",
    "                    if uni_pmi_score[w1] < uni_min:\n",
    "                        uni_min = uni_pmi_score[w1]\n",
    "                    if uni_pmi_score[w1] > 0:\n",
    "                        pos_pmi_num += 1\n",
    "                    elif uni_pmi_score[w1] < 0:\n",
    "                        neg_pmi_num += 1\n",
    "                    # Use of POS tags\n",
    "                    if pos1[1] in set(['JJ','JJR','JJS']):\n",
    "                        adj_pmi_sum += uni_pmi_score[w1]\n",
    "                    if pos1[1] in set(['NN','NNP','NNS']):\n",
    "                        nn_pmi_sum += uni_pmi_score[w1]\n",
    "                    if pos1[1] in set(['RB','RBR','RBP','RP']):\n",
    "                        adv_pmi_sum += uni_pmi_score[w1]\n",
    "                    if pos1[1] in set(['VB','VBD','VBG','VBN','VBP','VBZ']):\n",
    "                        vb_pmi_sum += uni_pmi_score[w1]\n",
    "                # Use of NRC lexicon\n",
    "                if w1 in NRC_uni_keys:\n",
    "                    nrc_uni_sum += NRC_uni_dict[w1]\n",
    "            for w1,w2 in zip(data[comb[0]][s][:-1], data[comb[0]][s][1:]):\n",
    "                # Use of bigrams PMI\n",
    "                if (w1,w2) in pmi_bi_list:\n",
    "                    bi_sum += pmi_bi_score[(w1,w2)]\n",
    "                    if pmi_bi_score[(w1,w2)] > bi_max:\n",
    "                        bi_max = pmi_bi_score[(w1,w2)]\n",
    "                    if pmi_bi_score[(w1,w2)] < bi_min:\n",
    "                        bi_min = pmi_bi_score[(w1,w2)]\n",
    "                # Use of NRC lexicon\n",
    "                if (w1,w2) in NRC_bi_keys:\n",
    "                    nrc_bi_sum += NRC_bi_dict[(w1,w2)]\n",
    "            feat.append([\n",
    "                         pos_num,\n",
    "                         neg_num,\n",
    "                         pos_pmi_num,\n",
    "                         neg_pmi_num,\n",
    "                         uni_prob_sum,\n",
    "                         uni_sum,\n",
    "                         uni_max,\n",
    "                         uni_min,\n",
    "                         bi_sum,\n",
    "                         bi_max,\n",
    "                         bi_min,\n",
    "                         adj_sum,\n",
    "                         adv_sum,\n",
    "                         vb_sum,\n",
    "                         nn_sum,\n",
    "                         adj_pmi_sum,\n",
    "                         adv_pmi_sum,\n",
    "                         vb_pmi_sum,\n",
    "                         nn_pmi_sum,\n",
    "                         nrc_uni_sum,\n",
    "                         nrc_bi_sum\n",
    "                        ])\n",
    "            label.append(comb[1])\n",
    "    return np.array(feat), np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ba5a2",
   "metadata": {},
   "source": [
    "Run several ML algorithms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95cb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X, y = get_features(traindata)\n",
    "# ss = StandardScaler().fit(X)\n",
    "# X = ss.transform(X)\n",
    "\n",
    "X_test, y_true = get_features(valdata)\n",
    "if TESTING:\n",
    "    X_test, y_true = get_features(testdata)\n",
    "# shuffle again\n",
    "p = np.random.permutation(X_test.shape[0])\n",
    "X_test = X_test[p]\n",
    "y_true = y_true[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2076dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier\n",
      "------------------------------\n",
      "The accuracy is: 0.7138665843113032\n",
      "\n",
      "The precision for positive is: 0.7167742904339991\n",
      "The recall for positive is: 0.7239496435613529\n",
      "The f1 score for positive is: 0.720344099003924\n",
      "\n",
      "The precision for negative is: 0.710789766407119\n",
      "The recall for negative is: 0.7034124862399749\n",
      "The f1 score for negative is: 0.7070818842870693\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Classifier\")\n",
    "print(\"------------------------------\")\n",
    "clf = SVC(C=0.25).fit(X,y)\n",
    "y_pred = clf.predict(X_test)\n",
    "# y_pred = clf.predict(ss.transform(X_test))\n",
    "\n",
    "print(\"The accuracy is:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for pos class\n",
    "print(\"The precision for positive is:\", precision_score(y_true, y_pred))\n",
    "print(\"The recall for positive is:\", recall_score(y_true, y_pred))\n",
    "print(\"The f1 score for positive is:\", f1_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for neg class\n",
    "print(\"The precision for negative is:\", precision_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The recall for negative is:\", recall_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The f1 score for negative is:\", f1_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3017ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "------------------------------\n",
      "The accuracy is: 0.6756485484867202\n",
      "\n",
      "The precision for positive is: 0.7090178259349877\n",
      "The recall for positive is: 0.6153496132261489\n",
      "The f1 score for positive is: 0.6588712951684936\n",
      "\n",
      "The precision for negative is: 0.6492392807745505\n",
      "The recall for negative is: 0.7381663783613776\n",
      "The f1 score for negative is: 0.690852895724483\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Classifier\")\n",
    "print(\"------------------------------\")\n",
    "clf = GaussianNB().fit(X,y)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"The accuracy is:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for pos class\n",
    "print(\"The precision for positive is:\", precision_score(y_true, y_pred))\n",
    "print(\"The recall for positive is:\", recall_score(y_true, y_pred))\n",
    "print(\"The f1 score for positive is:\", f1_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for neg class\n",
    "print(\"The precision for negative is:\", precision_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The recall for negative is:\", recall_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The f1 score for negative is:\", f1_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec410e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "------------------------------\n",
      "The accuracy is: 0.6670012353304509\n",
      "\n",
      "The precision for positive is: 0.6762523191094619\n",
      "The recall for positive is: 0.6634309115728804\n",
      "The f1 score for positive is: 0.6697802618482506\n",
      "\n",
      "The precision for negative is: 0.6577729796421962\n",
      "The recall for negative is: 0.6707029407139488\n",
      "The f1 score for negative is: 0.6641750369851281\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Classifier\")\n",
    "print(\"------------------------------\")\n",
    "clf = DecisionTreeClassifier(max_depth=7).fit(X,y)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"The accuracy is:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for pos class\n",
    "print(\"The precision for positive is:\", precision_score(y_true, y_pred))\n",
    "print(\"The recall for positive is:\", recall_score(y_true, y_pred))\n",
    "print(\"The f1 score for positive is:\", f1_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for neg class\n",
    "print(\"The precision for negative is:\", precision_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The recall for negative is:\", recall_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The f1 score for negative is:\", f1_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b762bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Classifier\n",
      "------------------------------\n",
      "The accuracy is: 0.7092340951204448\n",
      "\n",
      "The precision for positive is: 0.7542723511422917\n",
      "The recall for positive is: 0.6359775519490368\n",
      "The f1 score for positive is: 0.6900921658986174\n",
      "\n",
      "The precision for negative is: 0.6753685919112674\n",
      "The recall for negative is: 0.7851863500550401\n",
      "The f1 score for negative is: 0.7261489237929029\n"
     ]
    }
   ],
   "source": [
    "print(\"NN Classifier\")\n",
    "print(\"------------------------------\")\n",
    "clf = MLPClassifier(learning_rate='adaptive', learning_rate_init=0.01, max_iter=300).fit(X,y)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"The accuracy is:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for pos class\n",
    "print(\"The precision for positive is:\", precision_score(y_true, y_pred))\n",
    "print(\"The recall for positive is:\", recall_score(y_true, y_pred))\n",
    "print(\"The f1 score for positive is:\", f1_score(y_true, y_pred))\n",
    "print(\"\")\n",
    "\n",
    "## Stat for neg class\n",
    "print(\"The precision for negative is:\", precision_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The recall for negative is:\", recall_score(y_true, y_pred, pos_label=0))\n",
    "print(\"The f1 score for negative is:\", f1_score(y_true, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4ed60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
